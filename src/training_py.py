# -*- coding: utf-8 -*-
"""training.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W17EZzluPC68QvhniTPquEinygpbRRsu
"""

!pip install mlflow scikit-learn pandas pytest --quiet

import mlflow
import mlflow.sklearn

mlflow.set_tracking_uri("file:///content/mlruns")  # Local tracking inside Colab
mlflow.set_experiment("credit-risk-model")

import pandas as pd

df = pd.read_csv("with_proxy_target.csv")

# Assume target column is already created in previous step as 'is_high_risk'
df = df.dropna(subset=["is_high_risk"])  # Ensure no missing target
X = df.drop(columns=["CustomerId", "is_high_risk"])
y = df["is_high_risk"]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

def build_proxy_target(df):
    df["TransactionStartTime"] = pd.to_datetime(df["TransactionStartTime"])
    snapshot = df["TransactionStartTime"].max() + pd.Timedelta(days=1)

    rfm = df.groupby("CustomerId").agg({
        "TransactionStartTime": lambda x: (snapshot - x.max()).days,
        "TransactionId": "count",
        "Amount": "sum"
    }).rename(columns={
        "TransactionStartTime": "Recency",
        "TransactionId": "Frequency",
        "Amount": "Monetary"
    }).reset_index()

    scaler = StandardScaler()
    rfm_scaled = scaler.fit_transform(rfm[["Recency", "Frequency", "Monetary"]])

    kmeans = KMeans(n_clusters=3, random_state=42)
    rfm["Cluster"] = kmeans.fit_predict(rfm_scaled)

    # Define high-risk as the cluster with lowest Frequency + Monetary
    risk_scores = rfm.groupby("Cluster")[["Frequency", "Monetary"]].mean().sum(axis=1)
    high_risk_cluster = risk_scores.idxmin()

    rfm["is_high_risk"] = (rfm["Cluster"] == high_risk_cluster).astype(int)

    return rfm[["CustomerId", "is_high_risk"]]

# Apply to your raw data
df = pd.read_csv("data.csv")
proxy_labels = build_proxy_target(df)

# Merge it back into main dataset
df = df.merge(proxy_labels, on="CustomerId", how="left")

X, y = preprocess_data(df)

import pandas as pd
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Preprocessing: Clean and Prepare Features
def preprocess_data(df):
    # Drop rows with missing target
    df = df.dropna(subset=["is_high_risk"])

    # Drop ID columns (not useful for ML)
    df = df.drop(columns=["CustomerId", "TransactionId", "BatchId", "SubscriptionId"])

    # Separate target
    y = df["is_high_risk"]
    X = df.drop(columns=["is_high_risk"])

    # Encode categorical features (One-Hot Encoding)
    X = pd.get_dummies(X, drop_first=True)

    return X, y

# 2. Evaluation Metrics
def evaluate_model(y_true, y_pred, y_proba):
    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred),
        "recall": recall_score(y_true, y_pred),
        "f1_score": f1_score(y_true, y_pred),
        "roc_auc": roc_auc_score(y_true, y_proba)
    }

# 3. Load Data and Preprocess
df = pd.read_csv("data.csv")  # or your full path
X, y = preprocess_data(df)

# 4. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Model Training and Logging
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

for name, model in models.items():
    with mlflow.start_run(run_name=name):
        # Fit model
        model.fit(X_train, y_train)

        # Predict
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]

        # Evaluate
        metrics = evaluate_model(y_test, y_pred, y_proba)

        # Log metrics
        for k, v in metrics.items():
            mlflow.log_metric(k, v)

        # Log model
        mlflow.sklearn.log_model(model, "model")

        print(f"âœ… {name} logged to MLflow with metrics: {metrics}")